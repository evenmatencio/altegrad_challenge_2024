{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTeGraD 2023 Data Challenge \n",
    "## Molecule Retrieval with Natural Language Queries\n",
    "### Ã‰cole Polytechnique\n",
    "\n",
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Import evaluation metric\n",
    "from sklearn.metrics import label_ranking_average_precision_score # Use : label_ranking_average_precision_score(y_true, y_pred)\n",
    "\n",
    "# Loading token embedding dictionary\n",
    "token_embedding_dict = np.load(\"data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "\n",
    "# ??\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ??\n",
    "from dataloader import GraphTextDataset\n",
    "\n",
    "# ??\n",
    "from torch_geometric.data import Dataset \n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# ??  \n",
    "from Model import Model\n",
    "\n",
    "# ??\n",
    "from torch import optim\n",
    "\n",
    "# Counting each epoch training time\n",
    "import time\n",
    "\n",
    "# ??\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "# ??\n",
    "\n",
    "from dataloader import GraphDataset\n",
    "from dataloader import TextDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided Benchmark : DistilBERT + GCN + Cosine similarity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()\n",
    "def contrastive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer, nrows=10)\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer, nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Model(model_name=model_name, num_node_features=300, nout=768, nhid=300, graph_hidden_channels=300) # nout = bert model hidden dim\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "epoch = 0\n",
    "loss = 0\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH1-----\n",
      "-----EPOCH1----- done.  Validation loss:  4.646461009979248\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_checkpoints/model0.pt\n",
      "-----EPOCH2-----\n",
      "-----EPOCH2----- done.  Validation loss:  4.641589164733887\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_checkpoints/model1.pt\n",
      "-----EPOCH3-----\n",
      "-----EPOCH3----- done.  Validation loss:  4.627495765686035\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_checkpoints/model2.pt\n",
      "-----EPOCH4-----\n",
      "-----EPOCH4----- done.  Validation loss:  4.617728233337402\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_checkpoints/model3.pt\n",
      "-----EPOCH5-----\n",
      "-----EPOCH5----- done.  Validation loss:  4.603543758392334\n",
      "validation loss improoved saving checkpoint...\n",
      "checkpoint saved to: ./model_checkpoints/model4.pt\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    print('-----EPOCH{}-----'.format(i+1))\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        \n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = contrastive_loss(x_graph, x_text)   \n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += current_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = contrastive_loss(x_graph, x_text)   \n",
    "        val_loss += current_loss.item()\n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "    print('-----EPOCH'+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    if best_validation_loss==val_loss:\n",
    "        print('validation loss improoved saving checkpoint...')\n",
    "        save_path = os.path.join('./model_checkpoints/', 'model'+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (graph_encoder): GraphEncoder(\n",
       "    (relu): ReLU()\n",
       "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): GCNConv(300, 300)\n",
       "    (conv2): GCNConv(300, 300)\n",
       "    (conv3): GCNConv(300, 300)\n",
       "    (mol_hidden1): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (mol_hidden2): Linear(in_features=300, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_encoder): TextEncoder(\n",
       "    (bert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading best model...')\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "graph_model = model.get_graph_encoder()\n",
    "text_model = model.get_text_encoder()\n",
    "\n",
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids', nrows=10)\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer, nrows=10)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "test_loader = DataLoader(test_cids_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "graph_embeddings = []\n",
    "for batch in test_loader:\n",
    "    for output in graph_model(batch.to(device)):\n",
    "        graph_embeddings.append(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size, shuffle=False)\n",
    "text_embeddings = []\n",
    "for batch in test_text_loader:\n",
    "    for output in text_model(batch['input_ids'].to(device), \n",
    "                             attention_mask=batch['attention_mask'].to(device)):\n",
    "        text_embeddings.append(output.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('outputs/submission_dumb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('outputs/submission_dumb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.009874</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>-0.013406</td>\n",
       "      <td>-0.013982</td>\n",
       "      <td>-0.001774</td>\n",
       "      <td>-0.008939</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.010627</td>\n",
       "      <td>-0.006086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>-0.018349</td>\n",
       "      <td>-0.011680</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>-0.002469</td>\n",
       "      <td>-0.010352</td>\n",
       "      <td>-0.019198</td>\n",
       "      <td>-0.006520</td>\n",
       "      <td>-0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.015993</td>\n",
       "      <td>-0.014658</td>\n",
       "      <td>-0.024656</td>\n",
       "      <td>-0.021249</td>\n",
       "      <td>-0.018638</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>-0.016849</td>\n",
       "      <td>-0.021520</td>\n",
       "      <td>-0.016214</td>\n",
       "      <td>-0.009420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>-0.010506</td>\n",
       "      <td>-0.018623</td>\n",
       "      <td>-0.013427</td>\n",
       "      <td>-0.011262</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.009370</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.009089</td>\n",
       "      <td>-0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.016527</td>\n",
       "      <td>-0.007393</td>\n",
       "      <td>-0.012251</td>\n",
       "      <td>-0.012557</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>-0.005090</td>\n",
       "      <td>-0.021559</td>\n",
       "      <td>-0.014019</td>\n",
       "      <td>-0.003318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID         0         1         2         3         4         5         6  \\\n",
       "0   0 -0.009874 -0.009006 -0.015367 -0.013406 -0.013982 -0.001774 -0.008939   \n",
       "1   1 -0.003363 -0.011163 -0.018349 -0.011680 -0.014895 -0.002469 -0.010352   \n",
       "2   2 -0.015993 -0.014658 -0.024656 -0.021249 -0.018638 -0.005473 -0.016849   \n",
       "3   3 -0.003931 -0.010506 -0.018623 -0.013427 -0.011262 -0.001596 -0.009370   \n",
       "4   4 -0.016527 -0.007393 -0.012251 -0.012557 -0.013111 -0.001895 -0.005090   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.017372 -0.010627 -0.006086  \n",
       "1 -0.019198 -0.006520 -0.005887  \n",
       "2 -0.021520 -0.016214 -0.009420  \n",
       "3 -0.014890 -0.009089 -0.006868  \n",
       "4 -0.021559 -0.014019 -0.003318  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
